{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RawData(object):\n",
    "    def __init__(self, name):\n",
    "        self.name=name\n",
    "        self.data={}\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self.data[key] = value\n",
    "           \n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        guid,\n",
    "        sentence,\n",
    "        index=None,\n",
    "        target=None,\n",
    "        label=None,\n",
    "        POS=None,\n",
    "        FGPOS=None,\n",
    "        text_a_2=None,\n",
    "        text_b_2=None,\n",
    "    ):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.sentence = sentence\n",
    "        self.index = index\n",
    "        self.target = target\n",
    "        self.label = label\n",
    "        self.POS = POS\n",
    "        self.FGPOS = FGPOS\n",
    "        self.sen_2 = text_a_2\n",
    "        self.ids_2 = text_b_2\n",
    "    \n",
    "def _read_mohx(data_dir, set_type):\n",
    "    dataset = []\n",
    "    for k in tqdm(range(10), desc='K-fold'):\n",
    "        file_path = data_dir+str(k)+'.tsv'\n",
    "        with open(file_path, encoding='utf8') as f:\n",
    "            lines = csv.reader(f, delimiter='\\t')\n",
    "            next(lines)\n",
    "            w_index = 0\n",
    "            flag = True\n",
    "            for line in lines:                \n",
    "                sen_id = line[0]\n",
    "                sentence = line[2]\n",
    "                label = line[1]\n",
    "                POS = line[3]\n",
    "                FGPOS = line[4]\n",
    "                ind = line[-1]\n",
    "\n",
    "                index = int(ind)\n",
    "                word = sentence.split()[index]\n",
    "                guid = \"%s-%s-%s\" % (set_type, str(k), sen_id)\n",
    "\n",
    "                dataset.append(\n",
    "                    InputExample(guid=guid, sentence=sentence, index=index, target=word, label=label, POS=POS, FGPOS=FGPOS)\n",
    "                    )\n",
    "        break\n",
    "        print(file_path, len(dataset))\n",
    "    return dataset\n",
    "\n",
    "def load_mohx():\n",
    "    dataset_name = 'mohx'\n",
    "    data_dir = 'data/MOH-X/CLS'\n",
    "    dataset = RawData(dataset_name)\n",
    "    train_path = os.path.join(data_dir, 'train')\n",
    "    test_path = os.path.join(data_dir, 'test')\n",
    "    if dataset_name == 'trofi' or 'mohx':\n",
    "        dataset['test'] = _read_mohx(test_path, 'test')\n",
    "        dataset['train'] = _read_mohx(train_path, 'train')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "K-fold:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "K-fold:   0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "data = load_mohx()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_by_index(tokenizer, seq, index=None, no_flat=False):\n",
    "    seq = seq.split(' ')   # seq already being splited\n",
    "    tokens_ids = [[tokenizer.bos_token_id]]\n",
    "    for i,ele in enumerate(seq):\n",
    "        if i:    tokens_ids.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(' '+ele)))\n",
    "        else:    tokens_ids.append(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(ele)))\n",
    "    tokens_ids.append([tokenizer.eos_token_id])\n",
    "\n",
    "    if not index==None:\n",
    "        i_s = 0     #start index of target word\n",
    "        for i, ele in enumerate(tokens_ids):\n",
    "            i_e = i_s+len(ele)    #end index of target word\n",
    "            if i == index+1:\n",
    "                if not no_flat:\n",
    "                    tokens_ids = sum(tokens_ids, [])  # return a flat ids list\n",
    "                return tokens_ids, [i_s, i_e]\n",
    "            i_s += len(ele)\n",
    "    \n",
    "    if not no_flat:\n",
    "        tokens_ids = sum(tokens_ids, [])  # return a flat ids list\n",
    "    return tokens_ids\n",
    "\n",
    "def translate(data):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "    trans_data = RawData(data.name)\n",
    "\n",
    "    explainer = shap.Explainer(model, tokenizer)\n",
    "\n",
    "    cn_data_meta = []\n",
    "    cn_data_liter = []\n",
    "    for key in data.data.keys():\n",
    "        en_data = data[key]\n",
    "        for sample in tqdm(en_data):\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "            tokenized = tokenizer(sample.sentence,padding=True,truncation=True,max_length=512,return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**tokenized)\n",
    "            cn_sam = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            \n",
    "            shap_values = explainer([sample.sentence], fixed_context=1)\n",
    "            _, idx = tokenize_by_index(tokenizer, sample.sentence, index=sample.index)\n",
    "            idx = idx[0]-1\n",
    "            shap_atten = shap_values.values[0]\n",
    "            cor_idx = shap_atten[idx].argmax()\n",
    "            cor_target = tokenizer.decode(outputs[0][cor_idx+1], skip_special_tokens=True)\n",
    "            \n",
    "            if sample.label == '1':\n",
    "                cn_data_meta.append([sample.target, sample.index, sample.sentence, cor_idx, cor_target, cn_sam[0]])\n",
    "            else:\n",
    "                cn_data_liter.append([sample.target, sample.index, sample.sentence, cor_idx, cor_target, cn_sam[0]])\n",
    "    trans_data['meta'] = cn_data_meta\n",
    "    trans_data['liter'] = cn_data_liter\n",
    "    return trans_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [07:07<00:00,  6.67s/it]\n",
      "100%|██████████| 583/583 [1:04:17<00:00,  6.62s/it]\n"
     ]
    }
   ],
   "source": [
    "cn_data = translate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def back_translate(data):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "    bt_data = RawData(data.name)\n",
    "\n",
    "    explainer = shap.Explainer(model, tokenizer)\n",
    "\n",
    "    en_data_meta = []\n",
    "    en_data_liter = []\n",
    "    for key in data.data.keys():\n",
    "        cn_data = data[key]\n",
    "        for sample in tqdm(cn_data):\n",
    "            zh = sample[5]\n",
    "            idx = sample[3]\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "            model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "            tokenized = tokenizer(zh,padding=True,truncation=True,max_length=512,return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(**tokenized)\n",
    "            en_sam = tokenizer.batch_decode(outputs,skip_special_tokens=True)\n",
    "\n",
    "            shap_values = explainer([zh], fixed_context=1)\n",
    "            shap_atten = shap_values.values[0]\n",
    "            cor_idx = shap_atten[idx].argmax()\n",
    "            cor_target = tokenizer.decode(outputs[0][cor_idx+1], skip_special_tokens=True)\n",
    "\n",
    "            if re.sub(r'[,.?();!:]','',cor_target) == re.sub(r'[,.?();!:]','',sample[0]):\n",
    "                pass_bt = '1'\n",
    "            else:\n",
    "                pass_bt = '0'\n",
    "    \n",
    "            if key == 'meta':\n",
    "                en_data_meta.append([sample[0], sample[1], sample[2], sample[4], sample[3], sample[5], cor_target, cor_idx, en_sam[0], pass_bt])\n",
    "            else:\n",
    "                en_data_liter.append([sample[0], sample[1], sample[2], sample[4], sample[3], sample[5], cor_target, cor_idx, en_sam[0], pass_bt])\n",
    "    bt_data['bt_meta'] = en_data_meta\n",
    "    bt_data['bt_liter'] = en_data_liter\n",
    "    return bt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [34:58<00:00,  6.66s/it]\n",
      "100%|██████████| 332/332 [36:46<00:00,  6.65s/it]\n"
     ]
    }
   ],
   "source": [
    "bt_data = back_translate(cn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tsv(data, path, headline=None):\n",
    "    print(f'{path} len: {len(data)}')\n",
    "    with open(path, 'w') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        if headline:\n",
    "            writer.writerow(headline)\n",
    "        writer.writerows(data)\n",
    "\n",
    "def save_data(data, save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    for key in data.data.keys():\n",
    "        path = os.path.join(save_path, key+'.tsv')\n",
    "        save_tsv(data[key], path, ['target','idx','en','cn_target','cn_idx','cn','bt_target','bt_idx','bt','pass_bt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/MOH-X/opus/bt_meta.tsv len: 315\n",
      "data/MOH-X/opus/bt_liter.tsv len: 332\n"
     ]
    }
   ],
   "source": [
    "save_path = 'data/MOH-X/opus'\n",
    "save_data(bt_data, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shun",
   "language": "python",
   "name": "shun"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
